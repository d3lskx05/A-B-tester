import os
import time
import re
import math
import psutil
import numpy as np
import pandas as pd
from io import BytesIO
from typing import List, Tuple, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

import streamlit as st
from sentence_transformers import SentenceTransformer

# =========================================
# ============== –ù–ê–°–¢–†–û–ô–ö–ò ================
# =========================================

st.set_page_config(page_title="A/B-—Ç–µ—Å—Ç–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", layout="wide")

DEFAULT_DATASET_PATH = "/mnt/data/data6.xlsx"  # —Ç–≤–æ–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
DEFAULT_TOP_K = 5

# –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ (–º–æ–∂–Ω–æ –≤–≤–æ–¥–∏—Ç—å –ª—é–±—ã–µ HF id –≤—Ä—É—á–Ω—É—é)
PRESET_MODELS = [
    "intfloat/multilingual-e5-small",
    "intfloat/multilingual-e5-base",
    "BAAI/bge-m3",
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "sentence-transformers/all-MiniLM-L6-v2",
]

# –ü—Ä–∞–≤–∏–ª–∞ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é; –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤ UI)
# role: "query" | "passage"
FAMILY_PREFIX_RULES = [
    {
        "pattern": r"(^|/)intfloat/.*e5.*",
        "query_prefix": "query: ",
        "passage_prefix": "passage: ",
    },
    {
        "pattern": r"(^|/)BAAI/bge-.*",
        "query_prefix": "query: ",
        "passage_prefix": "document: ",
    },
    # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ (MiniLM/mpnet/‚Ä¶): –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
]

# =========================================
# =========== –ö–ï–®–ò / –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï ======
# =========================================

@st.cache_resource(show_spinner=False)
def get_model_cached(model_name: str) -> SentenceTransformer:
    return SentenceTransformer(model_name)

def detect_family_prefixes(model_name: str) -> Tuple[Optional[str], Optional[str]]:
    for rule in FAMILY_PREFIX_RULES:
        if re.search(rule["pattern"], model_name):
            return rule["query_prefix"], rule["passage_prefix"]
    return None, None  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –Ω–µ—Ç

def maybe_prefix(texts: List[str], prefix: Optional[str], add_prefix: bool) -> List[str]:
    if add_prefix and prefix:
        return [prefix + t for t in texts]
    return texts

def cosine_sim_matrix(vec: np.ndarray, mat: np.ndarray, mat_norms: np.ndarray) -> np.ndarray:
    # vec: (d,), mat: (N, d)
    vnorm = np.linalg.norm(vec) or 1e-10
    return (mat @ vec) / (mat_norms * vnorm)

def human_bytes(n_bytes: int) -> str:
    if n_bytes < 1024: return f"{n_bytes} B"
    for unit in ["KB","MB","GB","TB"]:
        n_bytes /= 1024.0
        if n_bytes < 1024.0:
            return f"{n_bytes:.1f} {unit}"
    return f"{n_bytes:.1f} PB"

def get_process_mem():
    proc = psutil.Process(os.getpid())
    rss = proc.memory_info().rss
    return rss

# =========================================
# ============ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ============
# =========================================

@st.cache_data(show_spinner=False)
def load_excel_any(path_or_bytes: bytes | str) -> pd.DataFrame:
    if isinstance(path_or_bytes, (bytes, bytearray)):
        df = pd.read_excel(BytesIO(path_or_bytes))
    else:
        df = pd.read_excel(path_or_bytes)
    # –û–∂–∏–¥–∞–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: phrase, topics*, comment (–∫–∞–∫ –≤ —Ç–≤–æ—ë–º –ø—Ä–æ–µ–∫—Ç–µ)
    # –°–æ–±–∏—Ä–∞–µ–º topics-–∫–æ–ª–æ–Ω–∫–∏:
    topic_cols = [c for c in df.columns if str(c).lower().startswith("topics")]
    if not topic_cols:
        # –ï—Å–ª–∏ –Ω–µ—Ç topics-–∫–æ–ª–æ–Ω–æ–∫ ‚Äî —Å–æ–∑–¥–∞–¥–∏–º –ø—É—Å—Ç—ã–µ
        df["topics"] = [[] for _ in range(len(df))]
    else:
        df["topics"] = df[topic_cols].astype(str).agg(
            lambda x: [v for v in x if v and v != "nan"], axis=1
        )
    if "phrase" not in df.columns:
        raise ValueError("–í Excel –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ 'phrase'")
    if "comment" not in df.columns:
        df["comment"] = ""

    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª—è
    df["phrase_full"] = df["phrase"]
    df["phrase_proc"] = df["phrase"].astype(str).str.lower().str.replace(r"\s+", " ", regex=True).str.strip()
    return df[["phrase", "phrase_proc", "phrase_full", "topics", "comment"]]

# =========================================
# ====== –ü–û–î–ì–û–¢–û–í–ö–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –ë–ê–ó–´ ======
# =========================================

@st.cache_data(show_spinner=False)
def compute_phrase_embeddings(
    df: pd.DataFrame,
    model_name: str,
    add_prefix: bool,
    custom_query_prefix: str | None,
    custom_passage_prefix: str | None,
    batch_size: int = 128,
) -> Dict[str, np.ndarray]:
    """
    –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±–∞–∑—ã —Ñ—Ä–∞–∑ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Ñ–ª–∞–≥–∏ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict: {"embeddings": (N, d), "norms": (N,), "dim": d}
    """
    model = get_model_cached(model_name)

    # –ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ + –∫–∞—Å—Ç–æ–º–Ω—ã–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ UI
    auto_q, auto_p = detect_family_prefixes(model_name)
    q_prefix = (custom_query_prefix if (custom_query_prefix is not None) else auto_q)
    p_prefix = (custom_passage_prefix if (custom_passage_prefix is not None) else auto_p)

    phrases = df["phrase_proc"].tolist()
    passages = maybe_prefix(phrases, p_prefix, add_prefix)

    embs: List[np.ndarray] = []
    for i in range(0, len(passages), batch_size):
        batch = passages[i:i+batch_size]
        batch_embs = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)
        embs.append(batch_embs.astype("float32"))

    if embs:
        emb_mat = np.vstack(embs)
        norms = np.linalg.norm(emb_mat, axis=1)
        norms[norms == 0] = 1e-10
    else:
        dim = model.get_sentence_embedding_dimension()
        emb_mat = np.zeros((0, dim), dtype="float32")
        norms = np.zeros((0,), dtype="float32")

    return {
        "embeddings": emb_mat,
        "norms": norms,
        "dim": emb_mat.shape[1] if emb_mat.size else model.get_sentence_embedding_dimension(),
    }

# =========================================
# =============== –ü–û–ò–°–ö ====================
# =========================================

def search_topk(
    query_text: str,
    df: pd.DataFrame,
    model_name: str,
    precomputed: Dict[str, np.ndarray],
    add_prefix: bool,
    custom_query_prefix: str | None,
    custom_passage_prefix: str | None,
    top_k: int = DEFAULT_TOP_K,
    hybrid_query: bool = True,
) -> Dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ + –º–µ—Ç—Ä–∏–∫–∏.
    hybrid_query=True: –∫–∞–∫ –≤ —Ç–≤–æ—ë–º –ø—Ä–æ–µ–∫—Ç–µ ‚Äî —Å–º–µ—à–∏–≤–∞–µ–º query —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –∏ –±–µ–∑.
    """
    model = get_model_cached(model_name)

    auto_q, auto_p = detect_family_prefixes(model_name)
    q_prefix = (custom_query_prefix if (custom_query_prefix is not None) else auto_q)

    phrase_embs = precomputed["embeddings"]
    phrase_norms = precomputed["norms"]

    if phrase_embs is None or phrase_embs.size == 0:
        return {"results": [], "latency_s": 0.0}

    t0 = time.time()
    # –ó–∞–ø—Ä–æ—Å—ã: —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –∏ –±–µ–∑ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    q_proc = str(query_text).lower().strip()
    q_pref = maybe_prefix([q_proc], q_prefix, add_prefix)[0] if q_prefix else q_proc

    q_emb_pref = model.encode(q_pref, convert_to_numpy=True, show_progress_bar=False).astype("float32")
    sims_pref = cosine_sim_matrix(q_emb_pref, phrase_embs, phrase_norms)

    if hybrid_query:
        q_emb_raw = model.encode(q_proc, convert_to_numpy=True, show_progress_bar=False).astype("float32")
        sims_raw = cosine_sim_matrix(q_emb_raw, phrase_embs, phrase_norms)
        sims = (sims_pref + sims_raw) / 2.0
        sims = np.nan_to_num(sims, neginf=0.0, posinf=0.0)
    else:
        sims = sims_pref

    idx = np.argsort(sims)[::-1][:top_k]
    results = [
        {
            "score": float(sims[i]),
            "phrase_full": df.iloc[i]["phrase_full"],
            "topics": df.iloc[i]["topics"],
            "comment": df.iloc[i]["comment"],
        }
        for i in idx
    ]
    latency = time.time() - t0

    return {"results": results, "latency_s": latency}

# =========================================
# ========= –°–ò–ú–£–õ–Ø–¶–ò–Ø –ù–ê–ì–†–£–ó–ö–ò ============
# =========================================

def simulate_users(
    n_users: int,
    n_requests_per_user: int,
    query_text: str,
    df: pd.DataFrame,
    model_name: str,
    precomputed: Dict[str, np.ndarray],
    add_prefix: bool,
    custom_query_prefix: str | None,
    custom_passage_prefix: str | None,
    top_k: int,
) -> Dict:
    """
    –ü—Ä–æ—Å—Ç–µ–π—à–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è ¬´–≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π¬ª –ø–æ—Ç–æ–∫–∞–º–∏.
    –ò–∑–º–µ—Ä—è–µ–º latency –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å.
    """
    latencies = []
    start_rss = get_process_mem()
    start_cpu = psutil.cpu_percent(interval=None)
    t0 = time.time()

    def one_call():
        r = search_topk(
            query_text=query_text,
            df=df,
            model_name=model_name,
            precomputed=precomputed,
            add_prefix=add_prefix,
            custom_query_prefix=custom_query_prefix,
            custom_passage_prefix=custom_passage_prefix,
            top_k=top_k,
        )
        return r["latency_s"]

    with ThreadPoolExecutor(max_workers=n_users) as ex:
        futures = []
        for _ in range(n_users * n_requests_per_user):
            futures.append(ex.submit(one_call))
        for fut in as_completed(futures):
            latencies.append(fut.result())

    total_time = time.time() - t0
    end_rss = get_process_mem()
    end_cpu = psutil.cpu_percent(interval=None)

    latencies = np.array(latencies) if latencies else np.array([0.0])

    return {
        "count": int(len(latencies)),
        "avg_ms": float(latencies.mean() * 1000),
        "p95_ms": float(np.percentile(latencies, 95) * 1000),
        "min_ms": float(latencies.min() * 1000),
        "max_ms": float(latencies.max() * 1000),
        "total_time_s": float(total_time),
        "cpu_start_pct": float(start_cpu),
        "cpu_end_pct": float(end_cpu),
        "mem_start": int(start_rss),
        "mem_end": int(end_rss),
    }

# =========================================
# =============== UI ======================
# =========================================

st.title("üî¨ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π A/B-—Ç–µ—Å—Ç–µ—Ä –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
st.caption("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞, —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è E5/BGE/MiniLM/mpnet –∏ –¥—Ä.")

# ---- –°–∞–π–¥–±–∞—Ä: –ó–∞–≥—Ä—É–∑–∫–∞ / –ú–µ—Ç—Ä–∏–∫–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è / –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è
with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")

    st.subheader("üì• –î–∞—Ç–∞—Å–µ—Ç")
    uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å Excel (.xlsx) —Å –∫–æ–ª–æ–Ω–∫–æ–π 'phrase'", type=["xlsx"])
    if uploaded is not None:
        df = load_excel_any(uploaded.read())
        st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(df)}")
    else:
        # –§–æ–ª–ª–±—ç–∫ –Ω–∞ –∑–∞—Ä–∞–Ω–µ–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        df = load_excel_any(DEFAULT_DATASET_PATH)
        st.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {DEFAULT_DATASET_PATH}. –°—Ç—Ä–æ–∫: {len(df)}")

    st.subheader("üß™ –õ—ë–≥–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    n_phrases = len(df)
    all_topics = sorted({t for row in df["topics"] for t in row}) if "topics" in df.columns else []
    st.write(f"- –ö–æ–ª-–≤–æ —Ñ—Ä–∞–∑: **{n_phrases}**")
    st.write(f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–º–∞—Ç–∏–∫: **{len(all_topics)}**")
    st.write(f"- –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã: **{df['phrase'].astype(str).str.len().mean():.1f}**")
    st.write(f"- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã: **{df['phrase'].astype(str).str.len().max()}**")

    st.subheader("üß∞ –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (–≤–∫–ª/–≤—ã–∫–ª)")
    show_debug = st.toggle("–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é", value=True,
                           help="–≠—Ç–æ –ª–µ–≥–∫–æ —É–±—Ä–∞—Ç—å –Ω–∞ –ø—Ä–æ–¥–µ ‚Äî –≤—ã–∫–ª—é—á–∏—Ç–µ –∏ —É–¥–∞–ª–∏—Ç–µ –±–ª–æ–∫–∏ sidebar.write().")

    st.subheader("üíª –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤")
    vm = psutil.virtual_memory()
    cpu_pct = psutil.cpu_percent(interval=None)
    rss = get_process_mem()
    st.write(f"CPU: **{cpu_pct:.1f}%**")
    st.write(f"RAM (process): **{human_bytes(rss)}**")
    st.write(f"RAM (system used): **{human_bytes(vm.used)} / {human_bytes(vm.total)} ({vm.percent:.1f}%)**")

# ---- –ì–ª–∞–≤–Ω–∞—è: –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
st.markdown("### üß† –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è A/B")
colA, colB = st.columns(2)

with colA:
    st.markdown("**–ú–æ–¥–µ–ª—å A**")
    model_a = st.selectbox("–í—ã–±–µ—Ä–∏/–≤–≤–µ–¥–∏ HF id –º–æ–¥–µ–ª–∏ A:", options=PRESET_MODELS, index=1, key="model_a", help="–ú–æ–∂–Ω–æ –¥–æ–ø–∏—Å–∞—Ç—å —Å–≤–æ–π id –≤—Ä—É—á–Ω—É—é.")
    model_a = st.text_input("–ò–ª–∏ —É–∫–∞–∂–∏ –¥—Ä—É–≥–æ–π HF id –¥–ª—è –º–æ–¥–µ–ª–∏ A:", value=model_a, key="model_a_text")
with colB:
    st.markdown("**–ú–æ–¥–µ–ª—å B**")
    model_b = st.selectbox("–í—ã–±–µ—Ä–∏/–≤–≤–µ–¥–∏ HF id –º–æ–¥–µ–ª–∏ B:", options=PRESET_MODELS, index=0, key="model_b", help="–ú–æ–∂–Ω–æ –¥–æ–ø–∏—Å–∞—Ç—å —Å–≤–æ–π id –≤—Ä—É—á–Ω—É—é.")
    model_b = st.text_input("–ò–ª–∏ —É–∫–∞–∂–∏ –¥—Ä—É–≥–æ–π HF id –¥–ª—è –º–æ–¥–µ–ª–∏ B:", value=model_b, key="model_b_text")

st.markdown("---")

# –ü—Ä–µ—Ñ–∏–∫—Å—ã –∏ —Ä–µ–∂–∏–º—ã
st.markdown("### üîñ –ü—Ä–µ—Ñ–∏–∫—Å—ã –∏ —Ä–µ–∂–∏–º—ã")
c1, c2, c3, c4 = st.columns([1,1,1,1])
with c1:
    add_prefix_a = st.toggle("A: add_prefix", value=True, key="add_prefix_a")
with c2:
    add_prefix_b = st.toggle("B: add_prefix", value=True, key="add_prefix_b")
with c3:
    hybrid_a = st.toggle("A: –≥–∏–±—Ä–∏–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å", value=True, key="hybrid_a", help="–°–º–µ—à–∏–≤–∞–µ—Ç query —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –∏ –±–µ–∑ ‚Äî –∫–∞–∫ –≤ —Ç–≤–æ—ë–º –ø—Ä–æ–µ–∫—Ç–µ.")
with c4:
    hybrid_b = st.toggle("B: –≥–∏–±—Ä–∏–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å", value=True, key="hybrid_b")

# –ö–∞—Å—Ç–æ–º–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
with st.expander("üß© –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)"):
    st.caption("–ï—Å–ª–∏ –æ—Å—Ç–∞–≤–∏—Ç—å –ø—É—Å—Ç—ã–º–∏ ‚Äî –ø–æ–¥—Å—Ç–∞–≤—è—Ç—Å—è –∞–≤—Ç–æ–ø—Ä–µ—Ñ–∏–∫—Å—ã –ø–æ —Å–µ–º–µ–π—Å—Ç–≤—É –º–æ–¥–µ–ª–∏ (E5/BGE).")
    colp1, colp2 = st.columns(2)
    with colp1:
        custom_q_a = st.text_input("A: query_prefix", value="", placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä, 'query: '")
        custom_p_a = st.text_input("A: passage_prefix", value="", placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä, 'passage: '")
    with colp2:
        custom_q_b = st.text_input("B: query_prefix", value="", placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä, 'query: '", key="q_b")
        custom_p_b = st.text_input("B: passage_prefix", value="", placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä, 'document: '", key="p_b")

def none_if_empty(s: str) -> Optional[str]:
    s = (s or "").strip()
    return s if s != "" else None

custom_q_a = none_if_empty(custom_q_a)
custom_p_a = none_if_empty(custom_p_a)
custom_q_b = none_if_empty(custom_q_b)
custom_p_b = none_if_empty(custom_p_b)

# –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ–¥ –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å
with st.spinner("–ì–æ—Ç–æ–≤–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–¥ –º–æ–¥–µ–ª—å A..."):
    precomputed_a = compute_phrase_embeddings(df, model_a, add_prefix_a, custom_q_a, custom_p_a)
with st.spinner("–ì–æ—Ç–æ–≤–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–¥ –º–æ–¥–µ–ª—å B..."):
    precomputed_b = compute_phrase_embeddings(df, model_b, add_prefix_b, custom_q_b, custom_p_b)

# –ü–æ–∫–∞–∑ ¬´–ª—ë–≥–∫–∏—Ö¬ª –º–µ—Ç—Ä–∏–∫ –ø–æ –º–æ–¥–µ–ª—è–º
mc1, mc2 = st.columns(2)
with mc1:
    st.markdown(f"**–ú–æ–¥–µ–ª—å A:** `{model_a}`")
    st.write(f"- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: **{precomputed_a['dim']}**")
    st.write(f"- –§—Ä–∞–∑ –≤ –±–∞–∑–µ: **{len(df)}**")
with mc2:
    st.markdown(f"**–ú–æ–¥–µ–ª—å B:** `{model_b}`")
    st.write(f"- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: **{precomputed_b['dim']}**")
    st.write(f"- –§—Ä–∞–∑ –≤ –±–∞–∑–µ: **{len(df)}**")

st.markdown("---")

# ====== –ü–æ–∏—Å–∫–æ–≤—ã–π –≤–≤–æ–¥ + —Ñ–∏–ª—å—Ç—Ä—ã ======
st.markdown("### üîé –ü–æ–∏—Å–∫ –∏ A/B —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ")
query = st.text_input("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å (query):", "")

top_k = st.slider("Top-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:", min_value=1, max_value=20, value=DEFAULT_TOP_K, step=1)

# –§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–º–∞—Ç–∏–∫–∞–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
all_topics_sorted = sorted({t for row in df["topics"] for t in row}) if "topics" in df.columns else []
selected_topics = st.multiselect("–§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–º–∞—Ç–∏–∫–∞–º:", all_topics_sorted, default=[])
if selected_topics:
    mask = df["topics"].apply(lambda ts: any(t in ts for t in selected_topics))
    df_filtered = df[mask].reset_index(drop=True)

    # –í—ã—Ä–µ–∑–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    idxs = np.where(mask.values)[0]
    precomputed_a_f = {
        "embeddings": precomputed_a["embeddings"][idxs] if len(idxs) else np.zeros((0, precomputed_a["dim"]), dtype="float32"),
        "norms": precomputed_a["norms"][idxs] if len(idxs) else np.zeros((0,), dtype="float32"),
        "dim": precomputed_a["dim"]
    }
    precomputed_b_f = {
        "embeddings": precomputed_b["embeddings"][idxs] if len(idxs) else np.zeros((0, precomputed_b["dim"]), dtype="float32"),
        "norms": precomputed_b["norms"][idxs] if len(idxs) else np.zeros((0,), dtype="float32"),
        "dim": precomputed_b["dim"]
    }
else:
    df_filtered = df
    precomputed_a_f = precomputed_a
    precomputed_b_f = precomputed_b

# ====== A/B –ø–æ–∏—Å–∫ ======
if query:
    # –î–æ/–ø–æ—Å–ª–µ —Ä–µ—Å—É—Ä—Å—ã ‚Äî –ª—ë–≥–∫–æ–µ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    cpu_before = psutil.cpu_percent(interval=None)
    mem_before = get_process_mem()

    res_a = search_topk(
        query_text=query,
        df=df_filtered,
        model_name=model_a,
        precomputed=precomputed_a_f,
        add_prefix=add_prefix_a,
        custom_query_prefix=custom_q_a,
        custom_passage_prefix=custom_p_a,
        top_k=top_k,
        hybrid_query=hybrid_a
    )
    res_b = search_topk(
        query_text=query,
        df=df_filtered,
        model_name=model_b,
        precomputed=precomputed_b_f,
        add_prefix=add_prefix_b,
        custom_query_prefix=custom_q_b,
        custom_passage_prefix=custom_p_b,
        top_k=top_k,
        hybrid_query=hybrid_b
    )

    cpu_after = psutil.cpu_percent(interval=None)
    mem_after = get_process_mem()

    ca, cb = st.columns(2)
    with ca:
        st.subheader("A ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        st.write(f"‚è±Ô∏è –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: **{res_a['latency_s']*1000:.1f} ms**")
        st.write(f"CPU Œî: **{max(0.0, cpu_after - cpu_before):.1f}%**, RAM Œî: **{human_bytes(max(0, mem_after - mem_before))}**")
        for item in res_a["results"]:
            with st.container():
                st.markdown(
                    f"""<div style="border:1px solid #e0e0e0;border-radius:12px;padding:12px;margin:8px 0;background:#fafafa">
                        <div style="font-weight:600">üß† {item['phrase_full']}</div>
                        <div style="font-size:13px;color:#666">üéØ Score: {item['score']:.3f}</div>
                        <div style="font-size:13px;color:#666">üîñ –¢–µ–º–∞—Ç–∏–∫–∏: {', '.join(item['topics']) if item['topics'] else '‚Äî'}</div>
                    </div>""",
                    unsafe_allow_html=True
                )

    with cb:
        st.subheader("B ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        st.write(f"‚è±Ô∏è –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: **{res_b['latency_s']*1000:.1f} ms**")
        st.write(f"CPU Œî: **{max(0.0, cpu_after - cpu_before):.1f}%**, RAM Œî: **{human_bytes(max(0, mem_after - mem_before))}**")
        for item in res_b["results"]:
            with st.container():
                st.markdown(
                    f"""<div style="border:1px solid #e0e0e0;border-radius:12px;padding:12px;margin:8px 0;background:#fafafa">
                        <div style="font-weight:600">üß† {item['phrase_full']}</div>
                        <div style="font-size:13px;color:#666">üéØ Score: {item['score']:.3f}</div>
                        <div style="font-size:13px;color:#666">üîñ –¢–µ–º–∞—Ç–∏–∫–∏: {', '.join(item['topics']) if item['topics'] else '‚Äî'}</div>
                    </div>""",
                    unsafe_allow_html=True
                )

    if show_debug:
        st.sidebar.write("### üßæ –û—Ç–ª–∞–¥–∫–∞ (–ø–æ –∑–∞–ø—Ä–æ—Å—É)")
        st.sidebar.write(f"–ú–æ–¥–µ–ª—å A: `{model_a}` | add_prefix={add_prefix_a} | hybrid={hybrid_a}")
        st.sidebar.write(f"–ú–æ–¥–µ–ª—å B: `{model_b}` | add_prefix={add_prefix_b} | hybrid={hybrid_b}")
        st.sidebar.write(f"CPU (before‚Üíafter): {cpu_before:.1f}% ‚Üí {cpu_after:.1f}%")
        st.sidebar.write(f"RAM (proc, before‚Üíafter): {human_bytes(mem_before)} ‚Üí {human_bytes(mem_after)}")

st.markdown("---")

# ====== –°–∏–º—É–ª—è—Ü–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ ======
st.markdown("### üß™ –ù–∞–≥—Ä—É–∑–æ—á–Ω—ã–π —Ç–µ—Å—Ç (–ø—Ä–æ—Å—Ç–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)")
col_s1, col_s2, col_s3, col_s4 = st.columns([1,1,1,2])
with col_s1:
    sim_model = st.selectbox("–ú–æ–¥–µ–ª—å –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏", options=[model_a, model_b], index=0)
with col_s2:
    sim_users = st.number_input("–ö–æ–ª-–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π", min_value=1, max_value=32, value=5, step=1)
with col_s3:
    sim_reqs = st.number_input("–ó–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è", min_value=1, max_value=50, value=3, step=1)
with col_s4:
    sim_query = st.text_input("–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏", value=query or "–∫–∞–∫ –æ–ø–ª–∞—Ç–∏—Ç—å –∑–∞–∫–∞–∑")

if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–∏–º—É–ª—è—Ü–∏—é"):
    if sim_model == model_a:
        pre = precomputed_a_f
        ap = add_prefix_a
        cq = custom_q_a
        cp = custom_p_a
    else:
        pre = precomputed_b_f
        ap = add_prefix_b
        cq = custom_q_b
        cp = custom_p_b

    with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã..."):
        stats = simulate_users(
            n_users=int(sim_users),
            n_requests_per_user=int(sim_reqs),
            query_text=sim_query,
            df=df_filtered,
            model_name=sim_model,
            precomputed=pre,
            add_prefix=ap,
            custom_query_prefix=cq,
            custom_passage_prefix=cp,
            top_k=top_k,
        )
    st.success("–ì–æ—Ç–æ–≤–æ!")

    colr1, colr2, colr3 = st.columns(3)
    with colr1:
        st.metric("–ó–∞–ø—Ä–æ—Å–æ–≤ –≤—Å–µ–≥–æ", stats["count"])
        st.metric("–°—Ä–µ–¥–Ω–µ–µ, ms", f"{stats['avg_ms']:.1f}")
        st.metric("p95, ms", f"{stats['p95_ms']:.1f}")
    with colr2:
        st.metric("min, ms", f"{stats['min_ms']:.1f}")
        st.metric("max, ms", f"{stats['max_ms']:.1f}")
        st.metric("–í—Ä–µ–º—è —Ç–µ—Å—Ç–∞, s", f"{stats['total_time_s']:.2f}")
    with colr3:
        st.metric("CPU start ‚Üí end", f"{stats['cpu_start_pct']:.1f}% ‚Üí {stats['cpu_end_pct']:.1f}%")
        st.metric("RAM start", human_bytes(stats["mem_start"]))
        st.metric("RAM end", human_bytes(stats["mem_end"]))

st.markdown("---")
st.caption("–ü–æ–¥—Å–∫–∞–∑–∫–∞: –Ω–∞ –ø—Ä–æ–¥–µ –º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –≤–µ—Å—å –æ—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥ (sidebar.write), –æ—Å—Ç–∞–≤–∏–≤ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏.")
